{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import os\n",
                "import sys\n",
                "import math\n",
                "import datetime\n",
                "import pathlib\n",
                "import numpy as np\n",
                "\n",
                "dir_path = pathlib.Path(os.path.abspath('')).resolve()\n",
                "print(dir_path)\n",
                "HOME_DIR = str(dir_path).split('/OpenFormat')[0]\n",
                "\n",
                "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
                "\n",
                "PROJ_SRC_DIR = f'{HOME_DIR}/OpenFormat'\n",
                "sys.path.insert(1, f'{PROJ_SRC_DIR}')\n",
                "from python.scripts.utils import *\n",
                "num_rows = 1000 * 1000\n",
                "num_cols = 20\n",
                "workload_list = ['core', 'geo', 'classic', 'log', 'ml', 'bi']\n",
                "scan_exec_pq = f'{HOME_DIR}/arrow-private/cpp/out/build/openformat-release/release/parquet-scan-columnbatch'"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predefined workloads, size and scan time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# generate csv files\n",
                "os.chdir(f'{PROJ_SRC_DIR}/benchmark/generator_v2')\n",
                "!pwd\n",
                "for wl in workload_list:\n",
                "    base_name = f'{wl}_r{num_rows}_c{num_cols}'\n",
                "    os.system(f'python3 gen_workloads.py \\\n",
                "        {wl} {num_rows} {num_cols} {base_name}')\n",
                "    os.system(f'cp -r {base_name} {dir_path} && \\\n",
                "        rm -r {base_name}')\n",
                "os.chdir(dir_path)\n",
                "!pwd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "pq_config = enumerate_config(f'{PROJ_SRC_DIR}/python/experiments/pq_default.json')\n",
                "orc_config = enumerate_config(f'{PROJ_SRC_DIR}/python/experiments/orc_default.json')\n",
                "pq_name_suffix = '_' + \"_\".join([str(i) for i in list(pq_config[0].values())]) + '.parquet'\n",
                "orc_name_suffix = '_' + \"_\".join([str(i) for i in list(orc_config[0].values())]) + '.orc'\n",
                "print(pq_config)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(f'{PROJ_SRC_DIR}/python')\n",
                "for wl in workload_list:\n",
                "    base_name = f'{wl}_r{num_rows}_c{num_cols}'\n",
                "    os.system(f'cp general/{base_name}/gen_data/{base_name}.csv data_gen')\n",
                "    os.system(f'parquet-fromcsv -s general/{base_name}/gen_data/{base_name}_arrow_schema.txt -i \\\n",
                "    general/{base_name}/gen_data/{base_name}.csv -o general/{base_name}.parquet')\n",
                "    # os.system(f'python3 scripts/parquet_exp.py {base_name} pq_default -r=false -o=false')\n",
                "    orc_schema = open(f'general/{base_name}/gen_data/{base_name}_orc_schema.txt', 'r').read()\n",
                "    os.system(f'{HOME_DIR}/orc/build/tools/src/csv-import \\\"{orc_schema}\\\" \\\n",
                "            general/{base_name}/gen_data/{base_name}.csv {base_name}.orc')\n",
                "    # os.system(f'mv {base_name}{pq_name_suffix} general/{base_name}.parquet')\n",
                "    os.system(f'mv {base_name}.orc general/{base_name}.orc')\n",
                "os.chdir(f'{PROJ_SRC_DIR}/python/general')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# running experiments\n",
                "os.system('rm outputs/stats.json')\n",
                "for i in range(10):\n",
                "    for wl in workload_list:\n",
                "        base_name = f'{wl}_r{num_rows}_c{num_cols}'\n",
                "        os.system('sync; echo 3 > /proc/sys/vm/drop_caches')\n",
                "        time.sleep(1)\n",
                "        pq_read = float(os.popen(f'''{scan_exec_pq} \\\n",
                "                --batch_size=1024 ./{base_name}.parquet''').read().split('\\n')[0].split(' ')[-2])\n",
                "        orc_read = float(os.popen(f'''{HOME_DIR}/orc/build/tools/src/orc-scan \\\n",
                "                -b 1024 ./{base_name}.orc''').read().split('\\n')[0].split(' ')[-1])\n",
                "        orc_size = os.path.getsize(f'{base_name}.orc')\n",
                "        pq_size = os.path.getsize(f'{base_name}.parquet')\n",
                "        output_stats = {}\n",
                "        output_stats['workload'] = wl\n",
                "        output_stats['i'] = i\n",
                "        output_stats['file'] = 'parquet'\n",
                "        output_stats['size'] = pq_size\n",
                "        output_stats['read_time'] = pq_read\n",
                "        parse_output(output_stats)\n",
                "        output_stats['file'] = 'orc'\n",
                "        output_stats['size'] = orc_size\n",
                "        output_stats['read_time'] = orc_read\n",
                "        parse_output(output_stats)\n",
                "collect_results()\n",
                "os.system('mv outputs/stats.csv ../outputs/{}_{}.csv'.format('general_exp', timestamp))"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predefined workloads, filter (select)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# generate filters and store in dir\n",
                "os.chdir(dir_path)\n",
                "for wl in workload_list:\n",
                "    base_name = f'{wl}_r{num_rows}_c{num_cols}'\n",
                "    filter_dir = f'{wl}_filters'\n",
                "    os.makedirs(filter_dir, exist_ok=True)\n",
                "    os.chdir(filter_dir)\n",
                "    os.system(f'rm -rf point*.csv range*.csv')\n",
                "    config_name = f'{PROJ_SRC_DIR}/benchmark/generator_v2/filter_config/{wl}_filter.yaml'\n",
                "    os.system(f'''python3 {PROJ_SRC_DIR}/benchmark/generator_v2/filter_generator.py \\\n",
                "    ../{base_name}/gen_data/{base_name}.csv ../{base_name}/configs/table_config.json \\\n",
                "        {config_name}''')\n",
                "    os.chdir(dir_path)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# running experiments\n",
                "import yaml\n",
                "import itertools\n",
                "os.system('rm outputs/stats.json')\n",
                "output_stats = {}\n",
                "for wl in workload_list:\n",
                "    print(f'begin wl: {wl}')\n",
                "    output_stats['wl'] = wl\n",
                "    base_name = f'{wl}_r{num_rows}_c{num_cols}'\n",
                "    filter_dir = f'{wl}_filters'\n",
                "    config_name = f'{PROJ_SRC_DIR}/benchmark/generator_v2/filter_config/{wl}_filter.yaml'\n",
                "    config = yaml.safe_load(open(config_name))\n",
                "    config_list = config['filter_config']\n",
                "    proj_type = 'one'\n",
                "    for (i, fmt, config) in itertools.product(range(5), ['parquet', 'orc'], config_list):\n",
                "        output_stats['format'] = fmt\n",
                "        output_stats['i'] = i\n",
                "        output_stats['proj_type'] = proj_type\n",
                "        expectation = config.get('selectivity_expectation')\n",
                "        output_stats['selectivity'] = expectation\n",
                "        range_exist = True\n",
                "        point_exist = True\n",
                "        try:\n",
                "            range_filters = pd.read_csv(f'{filter_dir}/range_{expectation}.csv')\n",
                "        except:\n",
                "            range_exist = False\n",
                "        try:\n",
                "            point_filters = pd.read_csv(f'{filter_dir}/point_{expectation}.csv')\n",
                "        except:\n",
                "            point_exist = False\n",
                "        if range_exist:\n",
                "            output_stats['filter_type'] = 'range'\n",
                "            for index, row in range_filters.iterrows():\n",
                "                val1 = np.int64(float(row['val1'])) if row['dtype'] == 'int' else row['val1']\n",
                "                val2 = np.int64(float(row['val2'])) if row['dtype'] == 'int' else row['val2']\n",
                "                os.system('sync; echo 3 > /proc/sys/vm/drop_caches')\n",
                "                if fmt == 'parquet':\n",
                "                    cmd = f\"parquet-filterscan-noeval -f \\\n",
                "                        {base_name}.parquet -i {row['col_idx']} -l {val1} -r \\\n",
                "                            {val2} -e true\"\n",
                "                    output_stats['time'] = os.popen(cmd).read().split('\\n')[0].split(' ')[-2]\n",
                "                    output_stats['time_preload'] = os.popen(cmd).read().split('\\n')[0].split(' ')[-2]\n",
                "                else:\n",
                "                    cmd = f\"{HOME_DIR}/orc/build/c++/test/FilterExp \\\n",
                "                    {base_name}.orc f{row['col_idx']} {row['dtype']} range \\\n",
                "                        {val1} {val2} {proj_type} e\"\n",
                "                    output_stats['time'] = float(os.popen(cmd).read().split('\\n')[0].split(' ')[-1])*1000\n",
                "                    output_stats['time_preload'] = float(os.popen(cmd).read().split('\\n')[0].split(' ')[-1])*1000\n",
                "                parse_output(output_stats)\n",
                "        if point_exist:\n",
                "            output_stats['filter_type'] = 'point'\n",
                "            for index, row in point_filters.iterrows():\n",
                "                val1 = np.int64(float(row['value'])) if row['dtype'] == 'int' else row['value']\n",
                "                val2 = np.int64(float(row['value'])) if row['dtype'] == 'int' else row['value']\n",
                "                os.system('sync; echo 3 > /proc/sys/vm/drop_caches')\n",
                "                if fmt == 'parquet':\n",
                "                    cmd = f\"parquet-filterscan-noeval -f \\\n",
                "                        {base_name}.parquet -i {row['col_idx']} -l {val1} -r \\\n",
                "                            {val2} -e true\"\n",
                "                    output_stats['time'] = os.popen(cmd).read().split('\\n')[0].split(' ')[-2]\n",
                "                    output_stats['time_preload'] = os.popen(cmd).read().split('\\n')[0].split(' ')[-2]\n",
                "                else:\n",
                "                    cmd = f\"{HOME_DIR}/orc/build/c++/test/FilterExp \\\n",
                "                    {base_name}.orc f{row['col_idx']} {row['dtype']} range \\\n",
                "                        {val1} {val2} {proj_type} e\"\n",
                "                    output_stats['time'] = float(os.popen(cmd).read().split('\\n')[0].split(' ')[-1])*1000\n",
                "                    output_stats['time_preload'] = float(os.popen(cmd).read().split('\\n')[0].split(' ')[-1])*1000\n",
                "                parse_output(output_stats)\n",
                "collect_results()\n",
                "os.system('mv outputs/stats.csv ../outputs/{}_{}.csv'.format(f'general_filter', timestamp))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.10"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
