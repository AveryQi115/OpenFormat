{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/OpenFormat/vector_data\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "dir_path = pathlib.Path(os.path.abspath('')).resolve()\n",
    "HOME_DIR = str(dir_path).split('/OpenFormat')[0]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "PROJ_SRC_DIR = f'{HOME_DIR}/OpenFormat'\n",
    "sys.path.insert(1, f'{PROJ_SRC_DIR}')\n",
    "from python.scripts.utils import *\n",
    "print(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.orc as po\n",
    "import pyarrow as pa\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zarr\n",
    "import xarray as xr\n",
    "\n",
    "import os\n",
    "\n",
    "def get_size(path):\n",
    "    if os.path.isfile(path):\n",
    "        # If the path is a file, use os.path.getsize\n",
    "        return os.path.getsize(path)\n",
    "    elif os.path.isdir(path):\n",
    "        # If the path is a directory, sum the sizes of all the files in it\n",
    "        total = 0\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for f in filenames:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                total += os.path.getsize(fp)\n",
    "        return total\n",
    "    else:\n",
    "        # If the path is neither a file nor a directory\n",
    "        print(\"The path is neither a file nor a directory\")\n",
    "        return None\n",
    "\n",
    "# old ones before I clean up duplicates\n",
    "# file_to_embed_col = {\n",
    "#                     'nielsr--datacomp-small-with-embeddings-and-cluster-labels.parquet' : ['clip_l14_embedding'],\n",
    "#                      'crumb--flan-t5-small-embed-refinedweb.parquet': ['encoding'],\n",
    "#                      'crumb--flan-t5-large-embed-refinedweb.parquet': ['encoding'],\n",
    "#                      'xwjzds--ag_news_embed_test.parquet': ['test'],\n",
    "#                      'justinian336--salvadoran-news-embedded.parquet': ['embedding'],\n",
    "#                      'davanstrien--ia_embedded.parquet': ['embeddings'],\n",
    "#                      'dhmeltzer--ELI5_embedded.parquet': ['embeddings'],\n",
    "#                      'sanjin7--embedding_dataset_distilbert_base_uncased_ad_subwords.parquet': ['mean_embedding'],\n",
    "#                      'lsb--openwebtext-all-minilm-l6-v2-embedding.parquet': ['embedding'],\n",
    "#                      'davanstrien--pytorchmodelmetadata_with_embeddings.parquet': ['embedding'],\n",
    "#                      'llm-book.parquet': ['embeddings'],\n",
    "#                      'tollefj.parquet': ['embedding'],\n",
    "#                      'github-embeddings-doy.parquet': ['embeddings'],\n",
    "#                      'cohere-en.parquet': ['emb'],\n",
    "#                      'cohere-simple.parquet': ['emb'],\n",
    "#                      'laion.parquet': ['text_embedding'],\n",
    "#                      }\n",
    "file_to_embed_col = {\n",
    "    'cohere-simple': ['emb'],\n",
    "    'crumb--flan_t5': ['encoding'],\n",
    "    'davanstrien--ia_embedded': ['embeddings'],\n",
    "    'davanstrien--pytorchmodelmetadata_with_embeddings': ['embedding'],\n",
    "    'dhmeltzer--eli5': ['embeddings'],\n",
    "    'justinian336--salvadoran-news-embedded': ['embedding'],\n",
    "    'KaiserML--SemanticScholar': ['embedding'],\n",
    "    'KShivendu--openai': ['openai'],\n",
    "    'laion': ['text_embedding'],\n",
    "    'llm-book': ['embeddings'],\n",
    "    'lsb--minilm': ['embedding'],\n",
    "    'lsb--openwebtext-all-minilm-l6-v2-embedding': ['embedding'],\n",
    "    'LukeSajkowski--eco': ['embeddings'],\n",
    "    'maiia--mcphrasy': ['embeddings'],\n",
    "    'Multimodal-Fatima--StanfordCars_train_embeddings': ['vision_embeddings'],\n",
    "    'nielsr--datacomp-small-with-embeddings-and-cluster-labels': ['clip_l14_embedding'],\n",
    "    'rjac--all_the_news': ['embedding'],\n",
    "    'sanjin7--embedding_dataset_distilbert_base_uncased_ad_subwords': ['mean_embedding'],\n",
    "    # 'tollefj': ['embedding'],\n",
    "    'victorych22--lamini': ['embeddings'],\n",
    "    'vjain--tax_embedding': ['embedding'],\n",
    "    'xwjzds--ag_news_embed_test': ['test'],\n",
    "    \n",
    "    'mushfirat--diffusiondb': ['embedding'], # https://www.kaggle.com/datasets/mushfirat/diffusiondb-metadata-with-prompt-embeddings\n",
    "    'alpayariyak--math_embedded': ['embedding'], # https://huggingface.co/datasets/alpayariyak/MATH_Embedded_Instructor-XL/tree/main\n",
    "    'milly233--gov_data': ['embedding'], # https://huggingface.co/datasets/milly233/govdatata_embedding/tree/main\n",
    "    'CShorten--ArXivML': ['embedding'], # https://huggingface.co/datasets/CShorten/ArXiv-ML-Abstract-Embeddings/tree/main\n",
    "    'juancopi81--yannic': ['embedding'], # https://huggingface.co/datasets/juancopi81/yannic_ada_embeddings/tree/main/data\n",
    "    'Multimodal-Fatima--VQAv2': ['embedding'], # https://huggingface.co/datasets/Multimodal-Fatima/VQAv2_sample_test_embeddings/tree/main\n",
    "    'Manzorq--hf_spsaces': ['embedding'], # https://huggingface.co/datasets/anzorq/hf-spaces-descriptions-embeddings\n",
    "    'nickmuchi--netflix': ['embedding'], # https://huggingface.co/datasets/nickmuchi/netflix-shows-mpnet-embeddings\n",
    "    'mitermix--mj5_clip': ['embedding'] #https://huggingface.co/datasets/mitermix/mj5-clip-l-14-oai-embeddings/tree/main\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('rm -f outputs/stats.json')\n",
    "output_stats = {}\n",
    "for fname, embed_name in file_to_embed_col.items():\n",
    "    embed_name = embed_name[0]\n",
    "    embed_name = 'embedding'\n",
    "    table = pq.read_table(f'hug_data_embed_only/{fname}.zstd.parquet', columns=[embed_name])\n",
    "    \n",
    "    cur = table.column(embed_name)\n",
    "    np.save(f'hug_data_embed_only/{fname}.v.npy', np.vstack(cur.to_numpy())) \n",
    "    np.save(f'hug_data_embed_only/{fname}.npy', cur.to_numpy()) \n",
    "    output_stats[f'numpy'] = get_size(f'hug_data_embed_only/{fname}.npy')\n",
    "    output_stats[f'numpy-v'] = get_size(f'hug_data_embed_only/{fname}.v.npy')\n",
    "    # print(type(cur.type))\n",
    "    while type(cur[0].type) == pa.lib.ListType and type(cur[0][0].type) == pa.lib.ListType:\n",
    "        cur = pa.compute.list_flatten(cur)\n",
    "    embedding_table = pa.table([cur], names=['embedding'])\n",
    "    pq.write_table(embedding_table, f'hug_data_embed_only/{fname}.zstd.parquet', compression='zstd')\n",
    "    output_stats[f'parquet-zstd'] = get_size(f'hug_data_embed_only/{fname}.zstd.parquet')\n",
    "    \n",
    "    \n",
    "    if cur[0][0].type == pa.uint8():\n",
    "        cur = cur.cast(pa.list_(pa.int32()))\n",
    "    embedding_table = pa.table([cur], names=['embedding'])\n",
    "    po.write_table(embedding_table, f'hug_data_embed_only/{fname}.zstd.orc', compression='zstd')\n",
    "    output_stats[f'orc-zstd'] = get_size(f'hug_data_embed_only/{fname}.zstd.orc')\n",
    "    \n",
    "    df = embedding_table.to_pandas()\n",
    "    # Convert lists in df to numpy arrays and pad them to have the same length\n",
    "    max_length = df['embedding'].str.len().max()\n",
    "    # df['embedding'] = df['embedding'].apply(lambda x: np.pad(x, (0, max_length - len(x))))\n",
    "    with h5py.File(f'hug_data_embed_only/{fname}.gzip.h5', 'w') as f:\n",
    "        # For each column, create a dataset and save the numpy arrays\n",
    "        for column in df.columns:\n",
    "            # Create a fixed length dataset with GZIP compression\n",
    "            ds = f.create_dataset(column, (len(df), max_length), dtype='float64', compression='gzip')\n",
    "            # Save the numpy arrays to the dataset\n",
    "            ds[:] = np.vstack(df[column].values)\n",
    "    output_stats['fname'] = fname\n",
    "    output_stats['hdf5-gzip'] = get_size(f'hug_data_embed_only/{fname}.gzip.h5')\n",
    "    \n",
    "    # Convert pandas dataframe to xarray dataset\n",
    "    ds = xr.Dataset.from_dataframe(df)\n",
    "    # Convert the DataFrame to a 2D numpy array\n",
    "    np_array = np.stack(df['embedding'].values)\n",
    "    # Create a multi-dimensional xarray DataArray\n",
    "    da = xr.DataArray(np_array, dims=['x', 'y'])\n",
    "    # if da's type is uint8, convert it to int8\n",
    "    if da.dtype == 'uint8':\n",
    "        da = da.astype('int8')\n",
    "    for c in ['zstd']:\n",
    "        compressor = zarr.Blosc(cname=c)\n",
    "        # Write to Zarr format\n",
    "        da.to_dataset(name='embedding').to_zarr(f'hug_data_embed_only/{fname}.{c}.zarr', encoding={\"embedding\": {\"compressor\": compressor}}, mode='w')\n",
    "        z = zarr.open(f'hug_data_embed_only/{fname}.pqchunk.{c}.zarr', shape=np_array.shape, chunks=(65536, None), compressor=zarr.Blosc(cname=c), dtype=np_array.dtype)\n",
    "        z[:] = np_array\n",
    "        da.to_dataset(name='embedding').to_zarr(f'hug_data_embed_only/{fname}.{c}-level1.zarr', encoding={\"embedding\": {\"compressor\": zarr.Blosc(cname=c, clevel=1)}}, mode='w')\n",
    "        da.to_dataset(name='embedding').to_zarr(f'hug_data_embed_only/{fname}.{c}-level1-bitshuffle.zarr', encoding={\"embedding\": {\"compressor\": zarr.Blosc(cname=c, clevel=1, shuffle=2)}}, mode='w')\n",
    "        output_stats[f'zarr-{c}'] = get_size(f'hug_data_embed_only/{fname}.{c}.zarr')\n",
    "        output_stats[f'zarr-{c}-level1'] = get_size(f'hug_data_embed_only/{fname}.{c}-level1.zarr')\n",
    "        output_stats[f'zarr-{c}-level1-bitshuffle'] = get_size(f'hug_data_embed_only/{fname}.{c}-level1-bitshuffle.zarr')\n",
    "        output_stats[f'zarr-{c}-pqchunk'] = get_size(f'hug_data_embed_only/{fname}.pqchunk.{c}.zarr')\n",
    "    parse_output(output_stats)\n",
    "collect_results()\n",
    "os.system('mv outputs/stats.csv outputs/{}_{}.csv'.format('hpc_file', timestamp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('send_email hdf5_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
