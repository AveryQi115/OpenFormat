{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data download, index build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import random\n",
    "import os\n",
    "import urllib.request\n",
    "proxy_handler = urllib.request.ProxyHandler({})  # empty dictionary means no proxy\n",
    "opener = urllib.request.build_opener(proxy_handler)\n",
    "urllib.request.install_opener(opener)\n",
    "executor = ThreadPoolExecutor(max_workers=100)\n",
    "for sample in range(107):\n",
    "    # os.system('wget https://datasets-documentation.s3.eu-west-3.amazonaws.com/laion/{:04d}.parquet'.format(sample))\n",
    "    executor.submit(urllib.request.urlretrieve, 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/laion/{:04d}.parquet'.format(sample), '/public/xinyu/laion_100m/{:04d}.parquet'.format(sample))\n",
    "    # urllib.request.urlretrieve('https://datasets-documentation.s3.eu-west-3.amazonaws.com/laion/{:04d}.parquet'.format(sample), 'laion_100m/{:04d}.parquet'.format(sample))\n",
    "executor.shutdown(wait=True)\n",
    "from autofaiss import build_index\n",
    "import numpy as np\n",
    "import time\n",
    "begin = time.time()\n",
    "index, index_infos = build_index('/public/xinyu/laion_100m/', save_on_disk=True, index_path='autofaiss_100m.index',\n",
    "                                 index_infos_path=\"autofaiss_100m.json\", file_format='parquet', \n",
    "                                 embedding_column_name='image_embedding')\n",
    "print(time.time() - begin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/OpenFormat/vector_data\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.orc as po\n",
    "import pyarrow as pa\n",
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import pathlib\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "dir_path = pathlib.Path(os.path.abspath('')).resolve()\n",
    "print(dir_path)\n",
    "HOME_DIR = str(dir_path).split('/OpenFormat')[0]\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "PROJ_SRC_DIR = f'{HOME_DIR}/OpenFormat'\n",
    "sys.path.insert(1, f'{PROJ_SRC_DIR}')\n",
    "from python.scripts.utils import *\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "executor = ThreadPoolExecutor(max_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_row_id_map(directory):\n",
    "    files = sorted(os.listdir(directory))\n",
    "    row_id_map = {}\n",
    "    total_rows = 0\n",
    "\n",
    "    for file in files:\n",
    "        if file.endswith('.parquet'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            num_rows = pq.read_metadata(file_path).num_rows\n",
    "            row_id_map[file_path] = (total_rows, total_rows + num_rows - 1)\n",
    "            total_rows += num_rows\n",
    "        elif file.endswith('.orc'):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            num_rows = po.ORCFile(file_path).nrows\n",
    "            row_id_map[file_path] = (total_rows, total_rows + num_rows - 1)\n",
    "            total_rows += num_rows\n",
    "\n",
    "    return row_id_map\n",
    "\n",
    "# return file and local row id\n",
    "def find_file(row_id, row_id_map):\n",
    "    for file_path, (start, end) in row_id_map.items():\n",
    "        if start <= row_id <= end:\n",
    "            return file_path, row_id - start\n",
    "    return None\n",
    "\n",
    "# Usage\n",
    "row_id_map = {}\n",
    "row_id_map['parquet'] = create_row_id_map('/s3-mnt/laion_100m')\n",
    "row_id_map['orc'] = create_row_id_map('/s3-mnt/laion_100m_orc')\n",
    "# row_id_map['parquet'] = create_row_id_map('./laion_100m')\n",
    "# row_id_map['orc'] = create_row_id_map('./laion_100m_orc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3 breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90345\n"
     ]
    }
   ],
   "source": [
    "from faiss import read_index\n",
    "import pyarrow.dataset as dataset\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "SCALE = 100 # Million\n",
    "K = 10\n",
    "QUERY_CNT = 3\n",
    "\n",
    "# scan_exec_pq = f'{HOME_DIR}/arrow-private/cpp/out/build/openformat-release/release/selection_scan'\n",
    "scan_exec_pq =  '/mnt/arrow-private/cpp/out/build/openformat-release/release/selection_scan_multi_files'\n",
    "scan_exec_orc =  '/mnt/orc/build/c++/test/SelectionScanMultiFiles'\n",
    "\n",
    "class LogReader:\n",
    "    def __init__(self, filename, string):\n",
    "        self.filename = filename\n",
    "        self.string = string\n",
    "        self.position = 0\n",
    "\n",
    "    def count_new_lines_with_string(self):\n",
    "        count = 0\n",
    "        with open(self.filename, 'r') as f:\n",
    "            f.seek(self.position)\n",
    "            for line in f:\n",
    "                if self.string in line:\n",
    "                    count += 1\n",
    "            self.position = f.tell()\n",
    "        return count\n",
    "\n",
    "# Use the class\n",
    "log_reader = LogReader('/root/s3_log/mountpoint-s3-2023-07-31T02-55-45Z.log', 'mountpoint_s3_client::s3_crt_client::get_object: new request')\n",
    "print(log_reader.count_new_lines_with_string())\n",
    "\n",
    "ds = dataset.dataset('0139.parquet', format='parquet')\n",
    "df = ds.to_table(use_threads=True, columns=['image_embedding']).to_pandas()\n",
    "# df = pq.read_table('0139.parquet', columns=['image_embedding'], use_threads=True, ).to_pandas()\n",
    "queries = np.vstack([np.array(x[0]) for x in df.values])  # Convert to 2D array\n",
    "index = read_index(f'autofaiss_{SCALE}m.index')\n",
    "os.system('rm -f outputs/stats.json')\n",
    "output_stats = {}\n",
    "# for batch_size in [512, 1024, 2048]:\n",
    "fmts = ['orc', 'parquet']\n",
    "# fmts = ['orc']\n",
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "# for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "# for batch_size in [1, 2, 4, 8, 16, 32, 64]:\n",
    "    output_stats['batch_size'] = batch_size\n",
    "    for i in range(1):\n",
    "        for j in range(1):\n",
    "            batch_query = queries[j*batch_size:(j+1)*batch_size]\n",
    "            begin = time.time()\n",
    "            _, I = index.search(batch_query, K)\n",
    "            output_stats['vector_search_time'] = time.time() - begin\n",
    "            #output_stats['vector_search_result'] = np.array2string(I, separator=',')\n",
    "            output_stats['i'] = i\n",
    "            output_stats['query_id'] = j\n",
    "            for fmt in fmts:\n",
    "                # iterate each value in I, which is a 2-D numpy array\n",
    "                file_to_ids = {}\n",
    "                for _, value in np.ndenumerate(I):\n",
    "                    file_path, row_id = find_file(value, row_id_map[fmt])\n",
    "                    if file_path not in file_to_ids.keys():\n",
    "                        file_to_ids[file_path] = set()\n",
    "                    if fmt == 'orc':\n",
    "                        file_to_ids[file_path].add(row_id+row_id_map[fmt][file_path][0])\n",
    "                    else:\n",
    "                        file_to_ids[file_path].add(row_id)\n",
    "                for file_path, row_ids in file_to_ids.copy().items():\n",
    "                    file_to_ids[file_path] = ','.join([str(r) for r in sorted(list(row_ids))])\n",
    "                total_time = 0\n",
    "                total_bytes = 0\n",
    "                os.system('sync; echo 3 > /proc/sys/vm/drop_caches')\n",
    "                futures = []\n",
    "                begin = time.time()\n",
    "                serialized = json.dumps(file_to_ids)\n",
    "                with open('file_to_ids.json', 'w') as f:\n",
    "                    f.write(serialized)\n",
    "            #     for file_path, row_ids in file_to_ids.items():\n",
    "            #         futures.append(executor.submit(lambda: os.popen(f'''{scan_exec_pq} \\\n",
    "            # --columns=1,2 --rows={file_to_ids[file_path]} {file_path}''')))\n",
    "            #         output_lines = os.popen(f'''{scan_exec_pq} \\\n",
    "            # # --columns=1,2 --rows={','.join([str(r) for r in sorted(list(row_ids))])} {file_path}''').read().split('\\n')\n",
    "                    # total_time += float(output_lines[1].split(' ')[-2])\n",
    "                    # total_bytes += int(output_lines[3].split(' ')[-1])\n",
    "                # concurrent.futures.wait(futures)\n",
    "                if fmt == 'orc':\n",
    "                    output_lines = os.popen(f'''{scan_exec_orc} file_to_ids.json''').read().split('\\n') \n",
    "                    output_stats['time (s)_orc'] = float(output_lines[0].split(' ')[-2])\n",
    "                    output_stats['s3_gets_orc'] = log_reader.count_new_lines_with_string()\n",
    "                else:\n",
    "                    output_lines = os.popen(f'''{scan_exec_pq} file_to_ids.json''').read().split('\\n') \n",
    "                    output_stats['time (s)'] = float(output_lines[0].split(' ')[-2])\n",
    "                    output_stats['s3_gets'] = log_reader.count_new_lines_with_string()\n",
    "            # output_stats['time_py (s)'] = time.time() - begin\n",
    "            # output_stats['bytes'] = total_bytes\n",
    "            # output_stats['fmt'] = fmt\n",
    "            parse_output(output_stats)\n",
    "collect_results()\n",
    "os.system('mv outputs/stats.csv outputs/{}_{}.csv'.format(f'batch_vector_search_{SCALE}m_top{K}', timestamp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Massive exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faiss import read_index\n",
    "import pyarrow.dataset as dataset\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "SCALE = 100 # Million\n",
    "K = 10\n",
    "QUERY_CNT = 3\n",
    "\n",
    "# scan_exec_pq = f'{HOME_DIR}/arrow-private/cpp/out/build/openformat-release/release/selection_scan'\n",
    "scan_exec_pq =  '/mnt/arrow-private/cpp/out/build/openformat-release/release/selection_scan_multi_files'\n",
    "scan_exec_orc =  '/mnt/orc/build/c++/test/SelectionScanMultiFiles'\n",
    "\n",
    "ds = dataset.dataset('0139.parquet', format='parquet')\n",
    "df = ds.to_table(use_threads=True, columns=['image_embedding']).to_pandas()\n",
    "# df = pq.read_table('0139.parquet', columns=['image_embedding'], use_threads=True, ).to_pandas()\n",
    "queries = np.vstack([np.array(x[0]) for x in df.values])  # Convert to 2D array\n",
    "index = read_index(f'autofaiss_{SCALE}m.index')\n",
    "os.system('rm -f outputs/stats.json')\n",
    "output_stats = {}\n",
    "# for batch_size in [512, 1024, 2048]:\n",
    "# fmts = ['parquet']\n",
    "fmts = ['orc', 'parquet']\n",
    "# for batch_size in [64, 128, 256, 512, 1024]:\n",
    "# for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096]:\n",
    "for batch_size in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]:\n",
    "    output_stats['batch_size'] = batch_size\n",
    "    for i in range(1):\n",
    "        for j in range(QUERY_CNT):\n",
    "            batch_query = queries[j*batch_size:(j+1)*batch_size]\n",
    "            begin = time.time()\n",
    "            _, I = index.search(batch_query, K)\n",
    "            output_stats['vector_search_time'] = time.time() - begin\n",
    "            #output_stats['vector_search_result'] = np.array2string(I, separator=',')\n",
    "            output_stats['i'] = i\n",
    "            output_stats['query_id'] = j\n",
    "            for fmt in fmts:\n",
    "                # iterate each value in I, which is a 2-D numpy array\n",
    "                file_to_ids = {}\n",
    "                for _, value in np.ndenumerate(I):\n",
    "                    file_path, row_id = find_file(value, row_id_map[fmt])\n",
    "                    if file_path not in file_to_ids.keys():\n",
    "                        file_to_ids[file_path] = set()\n",
    "                    if fmt == 'orc':\n",
    "                        file_to_ids[file_path].add(row_id+row_id_map[fmt][file_path][0])\n",
    "                    else:\n",
    "                        file_to_ids[file_path].add(row_id)\n",
    "                for file_path, row_ids in file_to_ids.copy().items():\n",
    "                    file_to_ids[file_path] = ','.join([str(r) for r in sorted(list(row_ids))])\n",
    "                total_time = 0\n",
    "                total_bytes = 0\n",
    "                os.system('sync; echo 3 > /proc/sys/vm/drop_caches')\n",
    "                futures = []\n",
    "                begin = time.time()\n",
    "                serialized = json.dumps(file_to_ids)\n",
    "                with open('file_to_ids.json', 'w') as f:\n",
    "                    f.write(serialized)\n",
    "            #     for file_path, row_ids in file_to_ids.items():\n",
    "            #         futures.append(executor.submit(lambda: os.popen(f'''{scan_exec_pq} \\\n",
    "            # --columns=1,2 --rows={file_to_ids[file_path]} {file_path}''')))\n",
    "            #         output_lines = os.popen(f'''{scan_exec_pq} \\\n",
    "            # # --columns=1,2 --rows={','.join([str(r) for r in sorted(list(row_ids))])} {file_path}''').read().split('\\n')\n",
    "                    # total_time += float(output_lines[1].split(' ')[-2])\n",
    "                    # total_bytes += int(output_lines[3].split(' ')[-1])\n",
    "                # concurrent.futures.wait(futures)\n",
    "                if fmt == 'orc':\n",
    "                    output_lines = os.popen(f'''{scan_exec_orc} file_to_ids.json''').read().split('\\n') \n",
    "                    output_stats['time (s)_orc'] = float(output_lines[0].split(' ')[-2])\n",
    "                else:\n",
    "                    output_lines = os.popen(f'''{scan_exec_pq} file_to_ids.json''').read().split('\\n') \n",
    "                    output_stats['time (s)'] = float(output_lines[0].split(' ')[-2])\n",
    "            # output_stats['time_py (s)'] = time.time() - begin\n",
    "            # output_stats['bytes'] = total_bytes\n",
    "            # output_stats['fmt'] = fmt\n",
    "            parse_output(output_stats)\n",
    "collect_results()\n",
    "os.system('mv outputs/stats.csv outputs/{}_{}.csv'.format(f'batch_vector_search_{SCALE}m_top{K}', timestamp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
